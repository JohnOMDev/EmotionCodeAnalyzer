{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import spacy for NLP and re for regular expressions\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "# import sklearn transformers, models and pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the small language model from spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# set pandas text output to 400\n",
    "pd.options.display.max_colwidth = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin_phrase = pd.read_csv(\"../../Data/Prepared/CleanDatasets/fin_phrase_bank_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_fin_phrase['clean_text'], df_fin_phrase['label'], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train set: (3392, 300)\n"
     ]
    }
   ],
   "source": [
    "# Load the en_core_web_lg model\n",
    "nlp = spacy.load('en_core_web_lg', disable=[\"parser\", \"ner\"])\n",
    "docs_train = [nlp(str(doc)).vector for doc in X_train]\n",
    "X_train = np.vstack(docs_train)\n",
    "print('Shape of train set: {}'.format(X_train.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test set: (1454, 300)\n"
     ]
    }
   ],
   "source": [
    "docs_test = [nlp(doc).vector for doc in X_test]\n",
    "X_test = np.vstack(docs_test)\n",
    "print('Shape of test set: {}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_pipe = Pipeline([('estimator', LogisticRegression(C=1e3, solver='lbfgs', multi_class='multinomial', random_state=17, n_jobs=4, max_iter=10000))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.691\n"
     ]
    }
   ],
   "source": [
    "# cross validate\n",
    "print('F1 score: {:.3f}'.format(np.mean(cross_val_score(word2vec_pipe, X_train, y_train, scoring = 'f1_micro'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit pipeline\n",
    "word2vec_pipe.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "pred = word2vec_pipe.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_fin_phrase['clean_text'], df_fin_phrase['label'], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'text': X_test, 'truelabelNum': y_test, 'prediclabelNum': pred})\n",
    "# class 0: negative, class 1: neutral, class 2: positive\n",
    "df['pred_label'] = df['truelabelNum'].map({0: 'negative', 1: 'neutral', 2: 'positive'})\n",
    "df['true_label'] = df['prediclabelNum'].map({0: 'negative', 1: 'neutral', 2: 'positive'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../Data/Prepared/Word2vecAnalysis/FinancialWord2vecLogicReg.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral\n",
      "Neutral\n",
      "Neutral\n"
     ]
    }
   ],
   "source": [
    "sentences = ['growth is strong and we have plenty of liquidity.', \n",
    "               'there is a shortage of capital, and we need extra financing.', \n",
    "              'formulation patents might protect Vasotec to a limited extent.']\n",
    "for sen in sentences:\n",
    "    sentences_vector = [nlp(str(doc)).vector for doc in sentences]# vectorizing\n",
    "    if word2vec_pipe.predict(sentences_vector).prod() == 0:\n",
    "        print('Negative')\n",
    "    elif word2vec_pipe.predict(sentences_vector).prod() == 1:\n",
    "        print('Neutral')\n",
    "    else:\n",
    "        print('Positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = '../../Data/Models/FinancialLogitRec_word2vec.pt'\n",
    "pickle.dump(word2vec_pipe, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model with Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../../Data/Prepared/CleanDatasets/Tweet_train_clean.csv\")\n",
    "df_test = pd.read_csv(\"../../Data/Prepared/CleanDatasets/Tweet_valid_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dropna( inplace=True)\n",
    "df_test.dropna( inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train set: (16989, 300)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg', disable=[\"parser\", \"ner\"])\n",
    "docs_train = [nlp(str(doc)).vector for doc in df_train['clean_text']]\n",
    "X_train = np.vstack(docs_train)\n",
    "print('Shape of train set: {}'.format(X_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test set: (4117, 300)\n"
     ]
    }
   ],
   "source": [
    "docs_test = [nlp(doc).vector for doc in df_test['clean_text']]\n",
    "X_test = np.vstack(docs_test)\n",
    "print('Shape of test set: {}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['label']\n",
    "y_test = df_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.636\n"
     ]
    }
   ],
   "source": [
    "# cross validate\n",
    "print('F1 score: {:.3f}'.format(np.mean(cross_val_score(word2vec_pipe, X_train, y_train, scoring = 'f1_micro'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit pipeline\n",
    "word2vec_pipe.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "pred = word2vec_pipe.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.716298275443284\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           class  precision    recall  f1-score      support\n",
      "0              0   0.544118  0.506849  0.524823    73.000000\n",
      "1              1   0.748691  0.668224  0.706173   214.000000\n",
      "2              2   0.725934  0.752347  0.738905   852.000000\n",
      "3              3   0.671053  0.662338  0.666667    77.000000\n",
      "4              4   0.900990  0.938144  0.919192    97.000000\n",
      "5              5   0.851562  0.900826  0.875502   242.000000\n",
      "6              6   0.713333  0.732877  0.722973   146.000000\n",
      "7              7   0.764331  0.750000  0.757098   160.000000\n",
      "8              8   0.523810  0.687500  0.594595    32.000000\n",
      "9              9   0.606936  0.625000  0.615836   336.000000\n",
      "10            10   0.473684  0.692308  0.562500    13.000000\n",
      "11            11   0.062500  0.071429  0.066667    14.000000\n",
      "12            12   0.685484  0.714286  0.699588   119.000000\n",
      "13            13   0.629213  0.482759  0.546341   116.000000\n",
      "14            14   0.689737  0.696386  0.693046   415.000000\n",
      "15            15   0.690909  0.608000  0.646809   125.000000\n",
      "16            16   0.841880  0.791165  0.815735   249.000000\n",
      "17            17   0.706767  0.839286  0.767347   112.000000\n",
      "18            18   0.729128  0.744318  0.736645   528.000000\n",
      "19            19   0.664634  0.553299  0.603878   197.000000\n",
      "20      accuracy   0.716298  0.716298  0.716298     0.716298\n",
      "21     macro avg   0.661235  0.670867  0.663016  4117.000000\n",
      "22  weighted avg   0.716447  0.716298  0.715155  4117.000000\n"
     ]
    }
   ],
   "source": [
    "# Assuming y_test and y_pred are already defined\n",
    "report = classification_report(y_test, pred, output_dict=True)\n",
    "\n",
    "# Convert the report dictionary to a DataFrame\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "# Optionally, you can reset the index to have the labels as a column\n",
    "report_df.reset_index(inplace=True)\n",
    "report_df.rename(columns={'index': 'class'}, inplace=True)\n",
    "\n",
    "print(report_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DDB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
