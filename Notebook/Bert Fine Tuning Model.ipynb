{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa2ab3d0",
   "metadata": {},
   "source": [
    "## Fine Tuning Bert Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfab5fbe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e13953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel,get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8db6fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "\n",
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bc9dbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../Data/Prepared/NormalAnalysis/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cbb2fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(f'{file_path}*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abcd9103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Data/Prepared/NormalAnalysis/fin_phrase_TF_IDF.csv',\n",
       " '../Data/Prepared/NormalAnalysis/fin_phrase_bank_clean.csv',\n",
       " '../Data/Prepared/NormalAnalysis/Tweet_train_clean.csv',\n",
       " '../Data/Prepared/NormalAnalysis/Tweet_valid_clean.csv',\n",
       " '../Data/Prepared/NormalAnalysis/fin_report_df.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f736ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../Data/Prepared/NormalAnalysis/Tweet_train_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af3e3268",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = pd.read_csv('../Data/Prepared/NormalAnalysis/Tweet_valid_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d9ab16fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val, df_test = train_test_split(df_valid, test_size = 0.5, random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d76cbbc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here are Thursday's biggest analyst calls: App...</td>\n",
       "      <td>0</td>\n",
       "      <td>thursday biggest analyst call apple amazon tes...</td>\n",
       "      <td>Analyst Update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Buy Las Vegas Sands as travel to Singapore bui...</td>\n",
       "      <td>0</td>\n",
       "      <td>buy la vega sand travel singapore build well f...</td>\n",
       "      <td>Analyst Update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Piper Sandler downgrades DocuSign to sell, cit...</td>\n",
       "      <td>0</td>\n",
       "      <td>piper sandler downgrade docusign sell citing e...</td>\n",
       "      <td>Analyst Update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Analysts react to Tesla's latest earnings, bre...</td>\n",
       "      <td>0</td>\n",
       "      <td>analyst react tesla latest earnings break next...</td>\n",
       "      <td>Analyst Update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netflix and its peers are set for a ‘return to...</td>\n",
       "      <td>0</td>\n",
       "      <td>netflix peer set return growth analyst say giv...</td>\n",
       "      <td>Analyst Update</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  Here are Thursday's biggest analyst calls: App...      0   \n",
       "1  Buy Las Vegas Sands as travel to Singapore bui...      0   \n",
       "2  Piper Sandler downgrades DocuSign to sell, cit...      0   \n",
       "3  Analysts react to Tesla's latest earnings, bre...      0   \n",
       "4  Netflix and its peers are set for a ‘return to...      0   \n",
       "\n",
       "                                          clean_text           topic  \n",
       "0  thursday biggest analyst call apple amazon tes...  Analyst Update  \n",
       "1  buy la vega sand travel singapore build well f...  Analyst Update  \n",
       "2  piper sandler downgrade docusign sell citing e...  Analyst Update  \n",
       "3  analyst react tesla latest earnings break next...  Analyst Update  \n",
       "4  netflix peer set return growth analyst say giv...  Analyst Update  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "da2f4529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = sorted(df_train.label.unique())\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3bdee4a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "2     3545\n",
       "18    2118\n",
       "14    1822\n",
       "9     1557\n",
       "5      987\n",
       "16     985\n",
       "1      837\n",
       "19     823\n",
       "7      624\n",
       "6      524\n",
       "15     501\n",
       "17     495\n",
       "12     487\n",
       "13     471\n",
       "4      359\n",
       "3      321\n",
       "0      255\n",
       "8      166\n",
       "10      69\n",
       "11      44\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bea8b62d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Analyst Update',\n",
       " 'Company | Product News',\n",
       " 'Currencies',\n",
       " 'Dividend',\n",
       " 'Earnings',\n",
       " 'Energy | Oil',\n",
       " 'Fed | Central Banks',\n",
       " 'Financials',\n",
       " 'General News | Opinion',\n",
       " 'Gold | Metals | Materials',\n",
       " 'IPO',\n",
       " 'Legal | Regulation',\n",
       " 'M&A | Investments',\n",
       " 'Macro',\n",
       " 'Markets',\n",
       " 'Personnel Change',\n",
       " 'Politics',\n",
       " 'Stock Commentary',\n",
       " 'Stock Movement',\n",
       " 'Treasuries | Corporate Debt']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(df_train.topic.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0956b94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, targets, tokenizer, max_len, include_raw_text=False):\n",
    "        self.text = text\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.include_raw_text = include_raw_text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.text[item])\n",
    "        target = self.targets[item]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens = True,\n",
    "            max_length = self.max_len,\n",
    "            return_token_type_ids = False,\n",
    "            return_attention_mask = True,\n",
    "            truncation = True,\n",
    "            padding = True,\n",
    "            return_tensors = 'pt',)\n",
    "\n",
    "        output = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }\n",
    "        if self.include_raw_text:\n",
    "            output['text_text'] = text\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8965c675",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifierModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Sentiment classification model based on BERT.\n",
    "\n",
    "    Args:\n",
    "        n_classes (int): Number of classes for sentiment classification.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifierModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pre_trained_model_ckpt,return_dict=False)\n",
    "        self.drop = nn.Dropout(p = 0.2)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the sentiment classifier model.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Input tensor of shape (batch_size, sequence_length).\n",
    "            attention_mask (torch.Tensor): Attention mask tensor of shape (batch_size, sequence_length).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, n_classes).\n",
    "\n",
    "        \"\"\"\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask= attention_mask\n",
    "        )\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d6d97a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert_model(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    \"\"\"\n",
    "    Trains the given model using the provided data loader and optimizer.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Model to train.\n",
    "        data_loader (DataLoader): DataLoader providing the training data.\n",
    "        loss_fn: Loss function to optimize.\n",
    "        optimizer: Optimizer for updating the model's parameters.\n",
    "        device: Device to use for training.\n",
    "        scheduler: Learning rate scheduler.\n",
    "        n_examples (int): Total number of training examples.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the model on the training data.\n",
    "        float: Average training loss.\n",
    "\n",
    "    \"\"\"\n",
    "    model=model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    \"\"\"  iterate over the batches provided by the data loader. Within each iteration, the batch tensors are moved to the appropriate device. The model performs a forward pass on the input tensors, and the predicted labels are obtained by taking the maximum value along the appropriate dimension. \"\"\"\n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs, dim = 1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == targets).cpu()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions/n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6d8a59e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bert_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            _,preds = torch.max(outputs, dim = 1)\n",
    "\n",
    "            loss = loss_fn(outputs, targets.detach())\n",
    "            correct_predictions += torch.sum(preds == targets).cpu()\n",
    "            losses.append(loss.item())\n",
    "    return correct_predictions/n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e51c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_predictions(model, data_loader):\n",
    "    model = model.eval()\n",
    "    review_texts = []\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    real_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            texts = d[\"clean_text\"]\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            probs = F.softmax(outputs, dim =1)\n",
    "            review_texts.extend(texts)\n",
    "            predictions.extend(preds)\n",
    "            prediction_probs.extend(probs)\n",
    "            real_values.extend(targets)\n",
    "\n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "    return review_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a95697b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vocab.txt: 100%|███████████████████████████████████████| 232k/232k [00:00<00:00, 1.28MB/s]\n",
      "tokenizer_config.json: 100%|████████████████████████████| 48.0/48.0 [00:00<00:00, 122kB/s]\n",
      "config.json: 100%|███████████████████████████████████████| 570/570 [00:00<00:00, 2.08MB/s]\n"
     ]
    }
   ],
   "source": [
    "pre_trained_model_ckpt = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pre_trained_model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f434e367",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d0dd7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b57a6227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10ef95970>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d2f394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e60ea38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorWithPadding(tokenizer=tokenizer, padding = 'longest' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ea373188",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d2ff25b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len = MAX_LEN, batch_size = BATCH_SIZE, include_raw_text = False ):\n",
    "    ds = TextDataset(\n",
    "        text=df.text.to_list(),\n",
    "        targets = df.label.to_list(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len,\n",
    "        include_raw_text=include_raw_text\n",
    "    )\n",
    "    return DataLoader(ds, batch_size=batch_size, collate_fn=collator )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "32c80c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = create_data_loader(df_train, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3ad36edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_loader = create_data_loader(df_val, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "aa3c9f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_loader = create_data_loader(df_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "aed1835b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SentimentClassifierModel(len(class_names))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f3611e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2182,  2024,  9432,  1005,  1055,  5221, 12941,  4455,  1024,\n",
      "          6207,  1010,  9733,  1010, 26060,  1010, 14412,  4630,  4313,  1010,\n",
      "          9986,  2271, 23773,  1010,  4654, 22500,  1004, 23713,  1025,  2062,\n",
      "         16770,  1024,  1013,  1013,  1056,  1012,  2522,  1013,  1053,  2361,\n",
      "          2078,  2620,  2290, 13668,  2581, 27225,   102],\n",
      "        [  101,  4965,  5869,  7136, 13457,  2004,  3604,  2000,  5264, 16473,\n",
      "          1010,  7051, 23054,  2758, 16770,  1024,  1013,  1013,  1056,  1012,\n",
      "          2522,  1013, 13109,  2015,  2475,  2860, 28311, 18682,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 11939,  5472,  3917,  2091, 24170,  2015,  9986,  2271, 23773,\n",
      "          2000,  5271,  1010,  8951,  8319, 10831, 13463,  5766,  6653, 16770,\n",
      "          1024,  1013,  1013,  1056,  1012,  2522,  1013,  1015,  6633,  3723,\n",
      "          2860,  8029, 18098,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 18288, 10509,  2000, 26060,  1005,  1055,  6745, 16565,  1010,\n",
      "          3338,  2091,  2054,  1005,  1055,  2279,  2005,  3751,  2482,  9338,\n",
      "         16770,  1024,  1013,  1013,  1056,  1012,  2522,  1013,  6448, 14490,\n",
      "          2575,  2860,  2692,  2575,  2226,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 20907,  1998,  2049, 12746,  2024,  2275,  2005,  1037,  1520,\n",
      "          2709,  2000,  3930,  1010,  1521, 18288,  2360,  1010,  3228,  2028,\n",
      "          4518,  6036,  1003, 14961, 16770,  1024,  1013,  1013,  1056,  1012,\n",
      "          2522,  1013, 16545, 17299,  2140,  2692,  2094,  2683,  2015,  2549,\n",
      "           102,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 23724,  2015,  7164, 16565,  2005,  2122,  2104,  4842, 14192,\n",
      "          2075, 15768,  2089,  4474,  2813,  2395, 16770,  1024,  1013,  1013,\n",
      "          1056,  1012,  2522,  1013,  6887,  5910,  2100,  2615,  3654,  6672,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 18862, 18739,  4862,  3676,  3676,  1010,  2758,  6661,  2064,\n",
      "          8320,  2062,  2084,  2322,  1003,  2013,  2182, 16770,  1024,  1013,\n",
      "          1013,  1056,  1012,  2522,  1013, 29061,  9331,  7361, 10623,  2226,\n",
      "          2692,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 18288, 10509,  2000, 20907,  1005,  1055,  2844,  4284,  1010,\n",
      "          2007,  2070,  7302,  2000,  1037,  4022,  3953,  2005,  1996,  4518,\n",
      "         16770,  1024,  1013,  1013,  1056,  1012,  2522,  1013,  1039,  4160,\n",
      "          3070, 22578,  6672,  2546,  2094,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  4965, 18178, 28588,  2004,  6661,  2298,  8702,  2012,  2122,\n",
      "          3798,  1010, 26236,  9818,  2758, 16770,  1024,  1013,  1013,  1056,\n",
      "          1012,  2522,  1013,  1043,  2243, 18927,  2546,  2615,  2595,  6460,\n",
      "          2361,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  5253,  6156,  2758,  2122,  3795, 15768,  2024,  2275,  2005,\n",
      "         16565, 10299,  1517,  1998,  3957,  2028,  2058,  3429,  1003, 14961,\n",
      "         16770,  1024,  1013,  1013,  1056,  1012,  2522,  1013, 16216,  2860,\n",
      "         18684,  2629,  7677, 13088,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 19920,  2252,  1024, 18288,  3333,  1037,  9129,  1997,  2047,\n",
      "          3964,  2006,  2943, 15768,  1012,  2182,  1005,  1055,  2073,  2057,\n",
      "          3233,  2006, 14635, 16770,  1024,  1013,  1013,  1056,  1012,  2522,\n",
      "          1013,  6904,  2243,  2278,  2595,  9096, 24798,  2480,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 12935,  2527,  2091, 24170,  2015,  6373,  1010,  2758,  3768,\n",
      "          1997,  5356, 11443,  4859,  2071,  3480,  2049, 11058,  4132, 16770,\n",
      "          1024,  1013,  1013,  1056,  1012,  2522,  1013,  4705,  2629,  2615,\n",
      "          2692,  4859, 24316,  2549,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2924,  1997,  2637,  2003,  2062,  7087,  4509,  2084, 16545,\n",
      "          5302, 16998,  1012,  2182,  1005,  1055,  2054,  2008,  2965,  2005,\n",
      "          2813,  2395,  1998,  1996,  4610, 16770,  1024,  1013,  1013,  1056,\n",
      "          1012,  2522,  1013,  1062,  2232,  2595,  3501,  2290, 24703,  2860,\n",
      "          3501,  2615,   102,     0,     0,     0,     0],\n",
      "        [  101,  2182,  2024,  9857,  1005,  1055,  5221, 12941,  4455,  1024,\n",
      "         18804,  1010,  9090,  4140,  2571,  1010,  6207,  1010, 26060,  1010,\n",
      "          4654, 22500,  1010, 20907,  1010,  3103, 15532,  1004, 23713,  1025,\n",
      "          2062, 16770,  1024,  1013,  1013,  1056,  1012,  2522,  1013,  1060,\n",
      "          6292,  2480,  2497, 21600,  2497, 25465,   102],\n",
      "        [  101, 25022,  3775,  7659,  7513,  3976,  4539,  1010,  8951,  3097,\n",
      "          3863,  2132, 11101,  2015, 16770,  1024,  1013,  1013,  1056,  1012,\n",
      "          2522,  1013,  1062,  2015,  2078, 13344,  6038,  2290,  2860,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  3863,  6872, 17324,  8913,  2003,  1037,  4965,  2004, 19396,\n",
      "          9592,  4125,  1010,  5253,  6156,  2758, 16770,  1024,  1013,  1013,\n",
      "          1056,  1012,  2522,  1013,  1062, 22203,  2232,  2575,  2581,  2140,\n",
      "          2615,  2549,  2080,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'targets': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n",
      "dict_keys(['input_ids', 'attention_mask', 'targets'])\n",
      "torch.Size([16, 47])\n",
      "torch.Size([16, 47])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "#Testing to see if the data loader works appropriately\n",
    "data = next(iter(train_data_loader))\n",
    "\n",
    "print(data)\n",
    "print(data.keys()) # dict_keys(['input_ids', 'attention_mask', 'targets'])\n",
    "\n",
    "print(data['input_ids'].shape) # torch.Size([16, 512])\n",
    "\n",
    "print(data['attention_mask'].shape) # torch.Size([16, 512])\n",
    "\n",
    "print(data['targets'].shape) # torch.Size([16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "80017164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0626, 0.0713, 0.0193, 0.0357, 0.0440, 0.0852, 0.0934, 0.0358, 0.0731,\n",
       "         0.0717, 0.0382, 0.0383, 0.0344, 0.0798, 0.0509, 0.0348, 0.0349, 0.0146,\n",
       "         0.0402, 0.0418],\n",
       "        [0.0587, 0.0753, 0.0171, 0.0353, 0.0401, 0.0516, 0.0963, 0.0282, 0.0578,\n",
       "         0.0536, 0.0790, 0.0528, 0.0379, 0.0915, 0.0812, 0.0233, 0.0287, 0.0135,\n",
       "         0.0304, 0.0475],\n",
       "        [0.0435, 0.0732, 0.0193, 0.0404, 0.0721, 0.0737, 0.0729, 0.0320, 0.0566,\n",
       "         0.0648, 0.0536, 0.0423, 0.0295, 0.0817, 0.0736, 0.0370, 0.0478, 0.0144,\n",
       "         0.0400, 0.0317],\n",
       "        [0.0682, 0.0451, 0.0163, 0.0305, 0.0631, 0.1142, 0.0844, 0.0364, 0.0565,\n",
       "         0.0635, 0.0515, 0.0416, 0.0324, 0.0840, 0.0515, 0.0333, 0.0302, 0.0157,\n",
       "         0.0375, 0.0443],\n",
       "        [0.0542, 0.0610, 0.0302, 0.0420, 0.0494, 0.0644, 0.0782, 0.0342, 0.0490,\n",
       "         0.0644, 0.0520, 0.0484, 0.0353, 0.0771, 0.0597, 0.0300, 0.0527, 0.0213,\n",
       "         0.0500, 0.0466],\n",
       "        [0.0703, 0.0648, 0.0215, 0.0494, 0.0443, 0.0441, 0.0806, 0.0382, 0.0353,\n",
       "         0.0532, 0.0698, 0.0519, 0.0395, 0.0800, 0.0652, 0.0343, 0.0456, 0.0150,\n",
       "         0.0474, 0.0494],\n",
       "        [0.0508, 0.0555, 0.0227, 0.0416, 0.0552, 0.0725, 0.0783, 0.0437, 0.0630,\n",
       "         0.0461, 0.0563, 0.0331, 0.0404, 0.0962, 0.0515, 0.0342, 0.0344, 0.0279,\n",
       "         0.0416, 0.0551],\n",
       "        [0.0625, 0.0592, 0.0213, 0.0422, 0.0552, 0.0742, 0.0607, 0.0371, 0.0779,\n",
       "         0.0558, 0.0498, 0.0434, 0.0357, 0.0908, 0.0522, 0.0375, 0.0483, 0.0178,\n",
       "         0.0463, 0.0323],\n",
       "        [0.0424, 0.0623, 0.0303, 0.0405, 0.0506, 0.0591, 0.0915, 0.0239, 0.0774,\n",
       "         0.0821, 0.0459, 0.0530, 0.0256, 0.0899, 0.0465, 0.0301, 0.0472, 0.0171,\n",
       "         0.0408, 0.0439],\n",
       "        [0.0640, 0.0724, 0.0257, 0.0439, 0.0489, 0.0785, 0.0722, 0.0482, 0.0712,\n",
       "         0.0548, 0.0374, 0.0320, 0.0292, 0.0728, 0.0590, 0.0318, 0.0410, 0.0235,\n",
       "         0.0494, 0.0441],\n",
       "        [0.0513, 0.0883, 0.0168, 0.0400, 0.0573, 0.0590, 0.0623, 0.0308, 0.0665,\n",
       "         0.0695, 0.0450, 0.0319, 0.0448, 0.0754, 0.0617, 0.0408, 0.0401, 0.0147,\n",
       "         0.0608, 0.0430],\n",
       "        [0.1050, 0.0576, 0.0212, 0.0423, 0.0382, 0.0726, 0.0838, 0.0366, 0.0484,\n",
       "         0.0616, 0.0578, 0.0437, 0.0262, 0.0763, 0.0569, 0.0416, 0.0320, 0.0215,\n",
       "         0.0383, 0.0386],\n",
       "        [0.0486, 0.0695, 0.0176, 0.0424, 0.0423, 0.0771, 0.0616, 0.0399, 0.0712,\n",
       "         0.0622, 0.0547, 0.0476, 0.0432, 0.0883, 0.0707, 0.0361, 0.0403, 0.0168,\n",
       "         0.0353, 0.0346],\n",
       "        [0.0512, 0.0688, 0.0140, 0.0343, 0.0599, 0.0873, 0.0716, 0.0273, 0.0611,\n",
       "         0.0701, 0.0431, 0.0408, 0.0339, 0.0890, 0.0782, 0.0353, 0.0439, 0.0181,\n",
       "         0.0362, 0.0359],\n",
       "        [0.0694, 0.0592, 0.0304, 0.0292, 0.0495, 0.0887, 0.0828, 0.0501, 0.0611,\n",
       "         0.0477, 0.0512, 0.0377, 0.0333, 0.0895, 0.0523, 0.0279, 0.0377, 0.0227,\n",
       "         0.0437, 0.0359],\n",
       "        [0.0628, 0.0717, 0.0177, 0.0278, 0.0546, 0.1013, 0.0728, 0.0363, 0.0608,\n",
       "         0.0647, 0.0570, 0.0284, 0.0336, 0.0863, 0.0563, 0.0245, 0.0441, 0.0136,\n",
       "         0.0499, 0.0359]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just An evaluation run of the model\n",
    "input_ids = data['input_ids'].to(device)\n",
    "attention_mask = data['attention_mask'].to(device)\n",
    "F.softmax(model(input_ids,attention_mask), dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d5ba6b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr= 1e-5)\n",
    "\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps = 0, \n",
    "    num_training_steps=total_steps)\n",
    "\n",
    "# For multi-class classification you would usually just use nn.CrossEntropyLoss \n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3ad5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/ 2\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/ {EPOCHS}')\n",
    "    print('-'*15)\n",
    "    train_acc, train_loss = train_bert_model(model, train_data_loader, loss_fn, optimizer, device, scheduler, len(df_train))\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "    val_acc, val_loss = eval_bert_model(model, val_data_loader, loss_fn, device, len(df_val))\n",
    "    print(f'Val loss {val_loss} accuracy {val_acc}')\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "  \n",
    "    if val_acc>best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "        best_accuracy = val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f72cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
